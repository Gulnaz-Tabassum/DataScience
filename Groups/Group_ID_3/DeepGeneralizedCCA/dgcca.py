# -*- coding: utf-8 -*-
"""dgcca.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15L_7jxf0KH81UjAO6waIbQso1nqML0kD

@author : Madiha Qureshi

# Deep Generalized Cannonical Correlation Analysis Implementaion for 3 views

---

**Introduction** - The implementaion of DGCCA for 3 views using pytorch.

---


**cca-zoo library** - It is a collection of variants of canonical correlation analysis for multiview data.
Here, cca-zoo package is used for the implemntaion of GCCA (Generalized Cannonical Correlation Analysis). It contatins the differentiable GCCA Loss method that takes the outputs of each view's network and solves the GCCA eigenproblem as in the research paper.


---
"""

#install the cca-zoo package
#!pip install cca-zoo

#importing required libraries -
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim 
from torch.utils.data import BatchSampler, SequentialSampler, RandomSampler
from cca_zoo.objectives import GCCA as GCCA_Loss
#GCCA_Loss().loss method takes the outputs of each view's network and solves the generalized CCA eigenproblem
from cca_zoo.wrapper import Wrapper as GCCA

"""---


##  Class DNN : Creates a new Deep Neural Network

**Parameters** :


*    **layer_size** - It is the list of size of each layer in the DNN starting from the input layer
*   **activation** - The type of activation function to be used. Choose from 'relu' , 'tanh' , 'sigmoid' . By default, sigmoid.

**Methods**

  

*   **forward(self, l)** : forward propogates input l into the DNN and returns the output


---
"""

class DNN(nn.Module):
    def __init__(self, layer_size, activation):
        super(DNN, self).__init__()
        layers = []
        self.activation = activation
        
        # Select activation function 
        if self.activation == 'relu':
          self.activation_func = nn.RelU()
        elif self.activation == 'tanh':
          self.activation_func = nn.Tanh()
        elif self.activation == 'sigmoid':
          self.activation_func = nn.Sigmoid()
        else:
          self.activation_func = nn.Sigmoid()
          #Defaults as Sigmoid

        for l_id in range(len(layer_size) - 1):
            if l_id == len(layer_size) - 2: #second last layer
                layers.append(nn.Sequential(
                    #Applys Normalizatiom
                    nn.BatchNorm1d(num_features=layer_size[l_id], affine=False), 
                    nn.Linear(layer_size[l_id], layer_size[l_id + 1]),
                ))
            else: #all other layers
                layers.append(nn.Sequential(
                    nn.Linear(layer_size[l_id], layer_size[l_id + 1]), 
                    #Adding activation function to the network
                    self.activation_func,
                    nn.BatchNorm1d(num_features=layer_size[l_id + 1], affine=False),                                     
                ))

        self.layers = nn.ModuleList(layers)

    #overriden
    def forward(self, l):
        for layer in self.layers:
            l = layer(l)
        return l

"""---


## Class : DGCCA_architecture - Defines the architecture for three DNNs

**Parameters**


*   **layer_size1 , layer_size2 , layer_size3** : list of sizes of each layer of first, second and third DNN(view) respectively staring from input layer.

**Methods**

  

*   **forward(self, x1, x2, x3)** : forward propogates x1 into the first DNN,x2 into the second DNN and x3 into the third DNN and returns the outputs.




---
"""

class DGCCA_architecture(nn.Module): # for three views
    def __init__(self, layer_size1, layer_size2, layer_size3, activation):
        super(DGCCA_architecture, self).__init__()
        self.activation = activation
        #creating 3 networks corresponding to each view
        self.model1 = DNN(layer_sizes1,activation)
        self.model2 = DNN(layer_sizes2,activation)
        self.model3 = DNN(layer_sizes2,activation)

    #overriden
    def forward(self, x1, x2, x3):
        output1 = self.model1(x1)
        output2 = self.model2(x2)
        output3 = self.model3(x3)

        return output1, output2, output3

"""---


## Class DGCCA : Implements the DGCCA Algorithm

**Parameters**


*   **architecture** : object of DGCCA_architecture class.
*   **learning_rate** : learning_rate of the network
*   **epoch_num** :How long to train the model.
*   **batch_size** : Number of example per minibatch.
*   **reg_param** :  the regularization parameter of the network
*   **out_size** : the size of the new space learned by the model (number of the new features)


**Methods**

  

*   **fit_transform(self, train_x1, train_x2, train_x3, test_x1, test_x2, test_x3)** -Learn and apply the dimension reduction on the train data batch-wise. Back propagates the ggca loss.Each view needs to have the same number of features as its corresponding view in the training data.
*   **predict(self, x1, x2, x3)** - returns gcca loss and output as list for given inputs x1, x2, x3 for view first, second, third respectively.
*   **test(self, x1, x2, x3)** - returns gcca loss mean and output as list for given inputs x1, x2, x3 for view first, second, third respectively.


---
"""

class DGCCA(nn.Module):
  def __init__(self, architecture, learning_rate, epoch_num, batch_size, reg_par, out_size:int):
        super(DGCCA, self).__init__()
        self.arch = architecture
        self.learning_rate = learning_rate
        self.reg_par = reg_par
        # Stochastic Gradient Descent used as optimizer like in the research paper
        self.optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=reg_par)
        self.epoch_num = epoch_num
        self.batch_size = batch_size
        self.out_size = out_size
        # The GCCA loss function - is back propogated to tune DNNs acc to input data
        self.loss_function = GCCA_Loss(self.out_size , self.reg_par).loss

  def predict(self, x1, x2, x3):
        #exception handling
        with torch.no_grad():
            self.arch.eval()
            data_size = x1.size(0)
            #making mini batches
            batch_idxs = list(BatchSampler(SequentialSampler(
                range(data_size)), batch_size=self.batch_size, drop_last=False))
            losses = []
            outputs1 = []
            outputs2 = []
            outputs3 = []
            for x in batch_idxs:
                batch_x1 = x1[x, :]
                batch_x2 = x2[x, :]
                batch_x3 = x3[x, :]
                #forward feeding to network
                o1, o2, o3 = self.arch(batch_x1, batch_x2, batch_x3)
                outputs1.append(o1)
                outputs2.append(o2)
                outputs3.append(o3)
                loss = self.loss_function(o1, o2, o3)
                losses.append(loss.item())
                #new features
        outputs = [torch.cat(outputs1, dim=0).numpy(),
                   torch.cat(outputs2, dim=0).numpy(),
                   torch.cat(outputs3, dim=0).numpy()]
        return losses, outputs

  def test(self, x1, x2, x3):
        with torch.no_grad():
            losses, outputs = self.predict(x1, x2, x3)
            return np.mean(losses), outputs

  def fit_transform(self, train_x1, train_x2, train_x3, test_x1, test_x2, test_x3):
      data_size = train_x1.size(0)
      train_losses = []
      for epoch in range(self.epoch_num):
          self.arch.train()
          batch_idxs = list(BatchSampler(RandomSampler(
              range(data_size)), batch_size=self.batch_size, drop_last=False))
          for x in batch_idxs:
              self.optimizer.zero_grad()           
              batch_x1 = train_x1[x, :]
              batch_x2 = train_x2[x, :]
              batch_x3 = train_x3[x, :] 
              o1, o2, o3 = self.arch(batch_x1, batch_x2, batch_x3)
              loss = self.loss_function(o1, o2, o3)
              train_losses.append(loss.data)
              #back propogating gcca loss
              loss.backward()
              self.optimizer.step()
          train_loss = np.mean(train_losses)
          print("Epcoh num: ",epoch, " Train loss = ", train_loss)

          # training the gcca model
          _, outputs = self.predict(train_x1, train_x2, train_x3)
          GCCA_obj = GCCA(self.out_size, method="gcca")
          GCCA_obj.fit(outputs[0], outputs[1], outputs[2],params=None) #function from cca-zoo

      test_loss,_ = self.test(test_x1, test_x2, test_x3)
      print("Test Loss = ",test_loss)
      print("Fitted Model to Data")

"""

 ---
## Example using Random Data

"""

X1 = torch.randn((200, 3))
X2 = torch.randn((200, 3))
X3 = torch.randn((200, 3))

T1 = torch.randn((20, 3))
T2 = torch.randn((20, 3))
T3 = torch.randn((20, 3))

layer_sizes1 = [3, 10, 10, 2]
layer_sizes2 = [3, 10, 10, 2]
layer_sizes3 = [3, 10, 10, 2]

model = DGCCA_architecture(layer_sizes1, layer_sizes2, layer_sizes3, "sigmoid")

learning_rate = 1e-3
epoch_num = 5
batch_size = 50
reg_par = 1e-5

algo = DGCCA(model, learning_rate, epoch_num, batch_size, reg_par, 20)
algo.fit_transform(X1,X2,X3,T1,T2,T3)

"""

---

---




"""